# 一、评估指标

#### 精确率公式

**precision = true-positives/(true-positives + false-positives)**

#### 召回率公式

**recall = true-positives/(true-positives + false-negatives)**



## F1 分数

既然我们已讨论了精确率和召回率，接下来可能要考虑的另一个指标是 F1 分数。F1 分数会同时考虑精确率和召回率，以便计算新的分数。

可将 F1 分数理解为精确率和召回率的加权平均值，其中 F1 分数的最佳值为 1、最差值为 0：

`F1 = 2 * (精确率 * 召回率) / (精确率 + 召回率)`

有关 F1 分数和如何在 sklearn 中使用它的更多信息，请查看此链接[此处](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)。



## 回归指标

正如前面对问题的回归类型所做的介绍，我们处理的是根据连续数据进行预测的模型。在这里，我们更关注预测的接近程度。

例如，对于身高和体重预测，我们不是很关心模型能否将某人的体重 100% 准确地预测到小于零点几磅，但可能很关心模型如何能始终进行接近的预测（可能与个人的真实体重相差 3-4 磅）



## 平均绝对误差

您可能已回想起，在统计学中可以使用绝对误差来测量误差，以找出预测值与真实值之间的差距。平均绝对误差的计算方法是，将各个样本的绝对误差汇总，然后根据数据点数量求出平均误差。通过将模型的所有绝对值加起来，可以避免因预测值比真实值过高或过低而抵销误差，并能获得用于评估模型的整体误差指标。

有关平均绝对误差和如何在 sklearn 中使用它的更多信息，请查看此链接[此处](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)。



## 均方误差

均方误差是另一个经常用于测量模型性能的指标。与绝对误差相比，残差（预测值与真实值的差值）被求平方。

对残差求平方的一些好处是，自动将所有误差转换为正数、注重较大的误差而不是较小的误差以及在微积分中是可微的（可让我们找到最小值和最大值）。

有关均方误差和如何在 sklearn 中使用它的更多信息，请查看此链接[此处](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)。



## 回归分数函数

除了误差指标之外，scikit-learn还包括了两个分数指标，范围通常从0到1，值0为坏，而值1为最好的表现，看起来和分类指标类似，都是数字越接近1.0分数就越好。

其中之一是[R2分数](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)，用来计算真值预测的可决系数。在 scikit-learn 里，这也是回归学习器默认的分数方法。

另一个是[可释方差分数](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score)

虽然眼下我们不会详细探讨这些指标，一个要记住的重点是，回归的默认指标是“分数越高越好”；即，越高的分数表明越好的表现。而当我们用到前面讲的误差指标时，我们要改变这个设定。



# 二、误差原因

## 误差原因

我们已讨论了一些用于测量模型性能的基本指标，现在来关注一下模型起初为何会出现误差。

在模型预测中，模型可能出现的误差来自两个主要来源，即：因模型无法表示基本数据的复杂度而造成的**偏差（bias）**，或者因模型对训练它所用的有限数据过度敏感而造成的**方差（variance）**。我们会对两者进行更详细的探讨。



## 偏差造成的误差 - 准确率和欠拟合

如前所述，如果模型具有足够的数据，但因不够复杂而无法捕捉基本关系，则会出现偏差。这样一来，模型一直会系统地错误表示数据，从而导致准确率降低。这种现象叫做**欠拟合（underfitting）**。

简单来说，如果模型不适当，就会出现偏差。举个例子：如果对象是按颜色和形状分类的，但模型只能按颜色来区分对象和将对象分类（模型过度简化），因而一直会错误地分类对象。

或者，我们可能有本质上是多项式的连续数据，但模型只能表示线性关系。在此情况下，我们向模型提供多少数据并不重要，因为模型根本无法表示其中的基本关系，我们需要更复杂的模型。



## 方差造成的误差 - 精度和过拟合

在训练模型时，通常使用来自较大训练集的有限数量样本。如果利用随机选择的数据子集反复训练模型，可以预料它的预测结果会因提供给它的具体样本而异。在这里，**方差（variance）**用来测量预测结果对于任何给定的测试样本会出现多大的变化。

出现方差是正常的，但方差过高表明模型无法将其预测结果泛化到更多的数据。对训练集高度敏感也称为**过拟合（overfitting）**，而且通常出现在模型过于复杂或我们没有足够的数据支持它时。

通常，可以利用更多数据进行训练，以降低模型预测结果的方差并提高精度。如果没有更多的数据可以用于训练，还可以通过限制模型的复杂度来降低方差。



## 学习曲线

现在你理解了偏差和方差的概念，让我们学习一下如何辨别模型表现的好坏。sklearn中的学习曲线函数可以帮到我们。它可以让我们通过数据点来了解模型表现的好坏。

可以先引入这个模块

```
from sklearn.learning_curve import learning_curve # sklearn 0.17
from sklearn.model_selection import learning_curve # sklearn 0.18

```

文档中一个合理的实现是：

```
 learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)

```

这里`estimator`是我们正在用来预测的模型，例如它可以是`GaussianNB()`，`X`和`y`是特征和目标。`cv`是交叉验证生成器，例如`KFold()`，'n_jobs'是平行运算的参数，`train_sizes`是多少数量的训练数据用来生成曲线。



## 改进模型的有效性

我们可以看到，在给定一组固定数据时，模型不能过于简单或复杂。如果过于简单，模型无法了解数据并会错误地表示数据。但是，如果建立非常复杂的模型，则需要更多数据才能了解基本关系，否则十分常见的是，模型会推断出在数据中实际上并不存在的关系。

关键在于，通过找出正确的模型复杂度来找到最大限度降低偏差和方差的最有效点。当然，数据越多，模型随着时间推移会变得越好。

要详细了解偏差和方差，建议阅读 Scott Fortmann-Roe 撰写的[这篇文章](http://scott.fortmann-roe.com/docs/BiasVariance.html)。



# 学习曲线与模型复杂度

## 学习曲线

让我们根据模型通过可视化图形从数据中学习的能力来探讨偏差与方差之间的关系。

机器学习中的学习曲线是一种可视化图形，能根据一系列训练实例中的训练和测试数据比较模型的指标性能。

在查看数据与误差之间的关系时，我们通常会看到，随着训练点数量的增加，误差会趋于下降。由于我们尝试构建从经验中学习的模型，因此这很有意义。

我们将训练集和测试集分隔开，以便更好地了解能否将模型泛化到未见过的数据而不是拟合到刚见过的数据。

在学习曲线中，当训练曲线和测试曲线均达到稳定阶段，并且两者之间的差距不再变化时，则可以确认模型已尽其所能地了解数据。



## 学习曲线

### 偏差

在训练误差和测试误差收敛并且相当高时，这实质上表示模型具有偏差。无论我们向其提供多少数据，模型都无法表示基本关系，因而出现系统性的高误差。

### 方差

如果训练误差与测试误差之间的差距很大，这实质上表示模型具有高方差。与偏差模型不同的是，如果有更多可供学习的数据，或者能简化表示数据的最重要特征的模型，则通常可以改进具有方差的模型。



## 理想的学习曲线

模型的最终目标是，误差小并能很好地泛化到未见过的数据（测试数据）。如果测试曲线和训练曲线均收敛，并且误差极低，就能看到这种模型。这种模型能根据未见过的数据非常准确地进行预测。



## 模型复杂度

与学习曲线图形不同，模型复杂度图形呈现的是模型复杂度如何改变训练曲线和测试曲线，而不是用以训练模型的数据点的数量。一般趋势是，随着模型增大，模型对固定的一组数据表现出更高的变化性。



## 学习曲线与模型复杂度

那么，学习曲线与模型复杂度之间有何关系？

如果我们获取具有同一组固定数据的相同机器学习算法的学习曲线，但为越来越高的模型复杂度创建几个图形，则所有学习曲线图形均代表模型复杂度图形。这就是说，如果我们获取了每个模型复杂度的最终测试误差和训练误差，并依据模型复杂度将它们可视化，则我们能够看到随着模型的增大模型的表现有多好。



## 模型复杂度的实际使用

既然知道了能通过分析模型复杂度图形来识别偏差和方差的问题，现在可利用一个可视化工具来帮助找出优化模型的方法。在下一部分中，我们会探讨 gridsearch 和如何微调模型以获得更好的性能。



## 摘要

恭喜您完成本课程！

小结：我们回顾了一些基本的统计学概念，还研究了几个指标，以根据手头的问题评估模型的学习表现。

接着，我们探讨了几种数据类型和分解数据的方式，以验证模型是否确实进行学习以泛化到未见过的数据，而不是泛化到给定的训练集。

然后，我们研究了模型可能会出现的两种常见误差：因未充分表示或欠拟合基本数据和方差而造成的偏差；以及过拟合训练数据且不再能很好泛化的模型复杂度。

最后，我们研究了模型复杂度，并使用了 gridsearch 来识别模型的最佳参数。



# 交叉验证

Sklearn 中的 GridSearchCV

GridSearchCV 用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。它的好处是，只需增加几行代码，就能遍历多种组合。

下面是来自 [sklearn 文档](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) 的一个示例：

```
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svr = svm.SVC()
clf = grid_search.GridSearchCV(svr, parameters)
clf.fit(iris.data, iris.target)

```

让我们逐行进行说明。

`parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}`

参数字典以及他们可取的值。在这种情况下，他们在尝试找到 kernel（可能的选择为 'linear' 和 'rbf' ）和 C（可能的选择为1和10）的最佳组合。

这时，会自动生成一个不同（kernel、C）参数值组成的“网格”:

| ('rbf', 1)    | ('rbf', 10)    |
| ------------- | -------------- |
| ('linear', 1) | ('linear', 10) |

各组合均用于训练 SVM，并使用交叉验证对表现进行评估。

`svr = svm.SVC()` 
这与创建分类器有点类似，就如我们从第一节课一直在做的一样。但是请注意，“clf” 到下一行才会生成—这儿仅仅是在说采用哪种算法。另一种思考方法是，“分类器”在这种情况下不仅仅是一个算法，而是算法加参数值。请注意，这里不需对 kernel 或 C 做各种尝试；下一行才处理这个问题。

`clf = grid_search.GridSearchCV(svr, parameters)` 
这是第一个不可思议之处，分类器创建好了。 我们传达算法 (svr) 和参数 (parameters) 字典来尝试，它生成一个网格的参数组合进行尝试。

`clf.fit(iris.data, iris.target)` 
第二个不可思议之处。 拟合函数现在尝试了所有的参数组合，并返回一个合适的分类器，自动调整至最佳参数组合。现在您便可通过 `clf.best_params_` 来获得参数值。



## 总结

恭喜你完成本课程！ 在本节课程中你学到了:

- 高效处理大量数据的数字工具
- 举例说明不同的数据类型，以及他们是如何产生。还提到了把他们用于标准工具的技巧
- 评价不同算法表现的一系列标准
- 优化算法表现基本的交叉验证技术
- 如何把学习过程和复杂度可视化，以及如何通过它选择高效的模型。

能到这一步你做的很好！我们现在把上述内容应用在一个实际项目中。你需要从数据集中做出预测，学习如何解释不通模型的优缺点，并找到一个最佳模型。

如果你觉得还是没有很好的理解GridSearch和交叉验证，可以参考论坛当中的两个帖子：

- [网格搜索和交叉验证的联系](http://discussions.youdaxue.com/t/topic/13868)
- [Grid Search](http://discussions.youdaxue.com/t/grid-search/6079)